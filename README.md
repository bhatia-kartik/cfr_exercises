My solutions to the exercises from [An Introduction to Counterfactual Regret Minimization, Neller &amp; Lanctot](https://www.ma.imperial.ac.uk/~dturaev/neller-lanctot.pdf) along with some commentary below.

### 2.5 - RPS Equilibrium
Code outputs the equilibrium strategies for both players, which approach [1/3, 1/3, 1/3], the Nash equilibrium. It also outputs a ternary plot to show the convergence of both strategies, a sample of which is attached below (note that when you run it the output may be different as it is not deterministic).

<p align="center"><img width="405" height="368" alt="image" src="https://github.com/user-attachments/assets/3bafd415-a817-4e16-a14b-7b1c0fd295a1" /></p>

### 2.6 - Colonel Blotto
This was an interesting one - before beginning, I tried to reason my way to a solution, and I figured the best strategy would be spreading out but not completely, maybe splitting it like (0, 3, 2). Then I started coding - accustomed to RPS, I was confused with the 21 possible actions in this one, but I created an array for the actions and worked through the steps in the algorithm from there. I decided to rewrite instead of referring to my RPS code to test my memory, and it produced neater and cleaner code (maybe my RPS code is too hacky on reflection)! The results were also harder to interpret so there's no graph this time, but I did include the relevant strategies, which are just those with a final average greater than 0.05. As expected, the Nash equilibrium is some variant of (0, 3, 2) and (1, 1, 3). 

But finding this was also an adventure, because initally trying 100k iterations, the relevant strategies were  [[0, 2, 3], [0, 4, 1], [1, 3, 1], [3, 0, 2]] and [[0, 2, 3], [2, 1, 2], [2, 2, 1], [2, 3, 0], [4, 1, 0], which seemed off as [4, 1, 0] was an easily counterable strategy. However, I couldn't do 1M iterations as even 100k took quite some time, so I had to look for performance improvements, or at least one big one. I realised that in RPS, since it's only 3 actions, getting the utility each round is inexpensive, but doing it for 21 actions adds up really quickly, and so I just calculated the payoff matrix once and indexed it instead of recalculating. This was a huge boost to performance (around 10x), and I could run 1M iterations, which gave me the answer I desired - the relevant strategies were now only [[0, 3, 2], [2, 3, 0]] and [[1, 1, 3], [3, 1, 1]] (each with ~50% chance). This is independent of their starting strategies (try it yourself!), although your answers may vary each run.
